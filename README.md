# DataEncryptionAndParalellComputingOnLogisticRegression
Szyfrowanie Danych i Obliczenia RÃ³wnolegÅ‚e w Regresji Logistycznej

## PrzeglÄ…d Projektu

Projekt demonstruje **uczenie maszynowe zachowujÄ…ce prywatnoÅ›Ä‡** przy uÅ¼yciu **w peÅ‚ni homomorficznego szyfrowania (FHE)** z modelami regresji logistycznej. ZaimplementowaliÅ›my podejÅ›cia sekwencyjne i rÃ³wnolegÅ‚e dla zaszyfrowanego wnioskowania, porÃ³wnujÄ…c wydajnoÅ›Ä‡ rÃ³Å¼nych zbiorÃ³w danych i frameworkÃ³w.

## Co ZrealizowaliÅ›my

### Implementacja Szyfrowania Homomorficznego
- **Schemat CKKS**: Wykorzystanie biblioteki TenSEAL do przybliÅ¼onego szyfrowania homomorficznego
- **Wnioskowanie ZachowujÄ…ce PrywatnoÅ›Ä‡**: Predykcje modelu na zaszyfrowanych danych bez koniecznoÅ›ci deszyfrowania
- **Bezpieczna Konfiguracja Kontekstu**: Optymalne parametry szyfrowania dla rÃ³wnowagi bezpieczeÅ„stwo-wydajnoÅ›Ä‡

### Framework ObliczeÅ„ RÃ³wnolegÅ‚ych
- **Pipeline Multiprocessingu**: Rozproszenie szyfrowania, ewaluacji i deszyfrowania miÄ™dzy rdzenie CPU
- **Optymalizacja WydajnoÅ›ci**: OsiÄ…gniÄ™cie znaczÄ…cych przyspieszeÅ„ na wiÄ™kszych zbiorach danych
- **RÃ³wnolegÅ‚oÅ›Ä‡ Oparta na Procesach**: OminiÄ™cie ograniczeÅ„ wÄ…tkowych TenSEAL poprzez niezaleÅ¼ne procesy robocze

### Implementacja w DwÃ³ch Frameworkach

#### Wersja PyTorch (`torch_project.ipynb`)
- Niestandardowy model regresji logistycznej z optymalizacjÄ… SGD
- **ZbiÃ³r Danych**: Breast Cancer Dataset (~188 prÃ³bek, 30 cech)
- **Wyniki**: 97.87% trafnoÅ›ci na danych jawnych, 88.83% na zaszyfrowanych
- **Obserwacja**: Przetwarzanie rÃ³wnolegÅ‚e wolniejsze ze wzglÄ™du na maÅ‚y zbiÃ³r danych

#### Wersja Scikit-Learn (`sklearn_covtype_project.ipynb`)
- Produkcyjna regresja logistyczna z wbudowanÄ… regularyzacjÄ…
- **ZbiÃ³r Danych**: Forest Cover Type Dataset (~30k prÃ³bek, 54 cechy)
- **Wyniki**: OsiÄ…gniÄ™to 3.6x przyspieszenie z przetwarzaniem rÃ³wnolegÅ‚ym
- **SkalowalnoÅ›Ä‡**: Wykazano korzyÅ›ci wydajnoÅ›ciowe na wiÄ™kszych zbiorach danych

## Kluczowe OsiÄ…gniÄ™cia Techniczne

### Analiza ZÅ‚oÅ¼onoÅ›ci Obliczeniowej
PrzeanalizowaliÅ›my zÅ‚oÅ¼onoÅ›Ä‡ algorytmicznÄ… kaÅ¼dego komponentu:

**Najkosztowniejsze Operacje:**
- `encrypt_data()`: O(nÂ·dÂ·N) - szyfrowanie dominuje koszt obliczeniowy
- `evaluate_encrypted()`: O(nÂ·dÂ·N) - zÅ‚oÅ¼onoÅ›Ä‡ zaszyfrowanego wnioskowania

**Funkcje RÃ³wnolegÅ‚e:**
- Teoretyczne przyspieszenie: O(X/p) gdzie p = liczba procesÃ³w
- Praktyczne korzyÅ›ci zaleÅ¼Ä… od rozmiaru danych i overhead'u komunikacji


## Struktura Projektu

```
â”œâ”€â”€ encrypted.py                   # Podstawowe funkcje szyfrowania/deszyfrowania
â”œâ”€â”€ torch_project.ipynb            # Implementacja PyTorch
â”œâ”€â”€ sklearn_covtype_project.ipynb  # Implementacja Scikit-learn  
â”œâ”€â”€ projekt-opis.ipynb             # SzczegÃ³Å‚owa analiza
â””â”€â”€ README.md                      # Ten plik
```

## Kluczowe Funkcje

### Pipeline Podstawowy (`encrypted.py`)
- `encrypt_chunk()`: RÃ³wnolegÅ‚e szyfrowanie danych
- `evaluate_chunk()`: Ewaluacja zaszyfrowanego modelu
- `parallel_encrypt()`: Rozproszone szyfrowanie miÄ™dzy procesami
- `parallel_evaluate()`: RÃ³wnolegÅ‚y pipeline wnioskowania

### Konfiguracja Kontekstu Szyfrowania
```python
context = ts.context(ts.SCHEME_TYPE.CKKS, 4096, 
                    coeff_mod_bit_sizes=[40, 20, 40])
context.global_scale = 2**20
context.generate_galois_keys()
```

## PrzykÅ‚ad UÅ¼ycia Pipeline'u

### Sequential Pipeline
```python
# 1. Trenowanie modelu
lr = train_model(X_train, y_train)
weight = lr.coef_[0]
bias = lr.intercept_[0]

# 2. Konfiguracja kontekstu HE
context = setup_he_context()

# 3. Szyfrowanie danych testowych
enc_X_test = encrypt_data(X_test, context)

# 4. Ewaluacja zaszyfrowanych danych
outputs = evaluate_encrypted(enc_X_test, weight, bias, context)

# 5. Deszyfrowanie wynikÃ³w
logits = decrypt_outputs(outputs)

# 6. Predykcja i ewaluacja
preds = predict_from_logits(logits)
acc = np.mean(preds == y_test)
```

### Parallel Pipeline
```python
# Serializacja kontekstu do multiprocessingu
ctx_serialized = context.serialize()

# 1. RÃ³wnolegÅ‚e szyfrowanie
enc_X_test_par = parallel_encrypt(X_test, ctx_serialized)

# 2. RÃ³wnolegÅ‚a ewaluacja
outputs_serialized = parallel_evaluate(enc_X_test_par, y_test, 
                                     ctx_serialized, weight, bias, context)

# 3. RÃ³wnolegÅ‚e deszyfrowanie
logits_par = parallel_decrypt(outputs_serialized, context)

# 4. Predykcja i ewaluacja
preds_par = predict_from_logits(logits_par)
acc_par = np.mean(preds_par == y_test)
```

## Wyniki BadaÅ„

### Optymalne Przypadki UÅ¼ycia
1. **Klasyfikacja Danych Medycznych**: Aplikacje krytyczne dla prywatnoÅ›ci
2. **Ocena Ryzyka Finansowego**: Wymagania zgodnoÅ›ci regulacyjnej  
3. **WdroÅ¼enia na DuÅ¼Ä… SkalÄ™**: Gdzie rozmiar zbioru danych uzasadnia overhead

### Progi WydajnoÅ›ciowe
- **Punkt RÃ³wnowagi RÃ³wnolegÅ‚oÅ›ci**: ~200-10,000 prÃ³bek w zaleÅ¼noÅ›ci od cech
- **Optymalna Liczba ProcesÃ³w**: Liczba rdzeni CPU (malejÄ…ce zwroty powyÅ¼ej)
- **WzglÄ™dy PamiÄ™ciowe**: Wektory CKKS wymagajÄ… znacznej iloÅ›ci RAM

### GÅ‚Ã³wne Obserwacje

#### PyTorch vs Scikit-Learn

| Framework | Zalety | Najlepsze dla |
|-----------|--------|---------------|
| **PyTorch** | â€¢ WiÄ™ksza elastycznoÅ›Ä‡ i kontrola<br>â€¢ Lepsza jakoÅ›Ä‡ modelu na maÅ‚ych zbiorach (97.87%)<br>â€¢ NiÅ¼szy czas ewaluacji na maÅ‚ych danych | â€¢ Niestandardowe architektury<br>â€¢ PeÅ‚na kontrola nad modelem<br>â€¢ Integracja z deep learning |
| **Scikit-Learn** | â€¢ Szybszy i prostszy w uÅ¼yciu<br>â€¢ Åšwietna wspÃ³Å‚praca z rÃ³wnolegÅ‚oÅ›ciÄ… (3.6x)<br>â€¢ Wbudowana regularyzacja | â€¢ Klasyczne problemy ML<br>â€¢ DuÅ¼e zbiory danych<br>â€¢ Szybkie prototypowanie |

#### Kiedy StosowaÄ‡ RÃ³wnolegÅ‚oÅ›Ä‡

| âœ… **OpÅ‚acalna gdy:** | âŒ **NieopÅ‚acalna gdy:** |
|----------------------|-------------------------|
| â€¢ nÂ·dÂ·N > prÃ³g (duÅ¼e dane)<br>â€¢ p â‰¤ liczba rdzeni CPU<br>â€¢ Czas obliczeÅ„ > czas komunikacji | â€¢ MaÅ‚e zbiory danych<br>â€¢ WiÄ™cej procesÃ³w niÅ¼ rdzeni<br>â€¢ Wysokie koszty serializacji |

## Wykorzystane Technologie

- **TenSEAL**: Szyfrowanie homomorficzne
- **PyTorch**: Framework gÅ‚Ä™bokiego uczenia
- **Scikit-Learn**: Klasyczne uczenie maszynowe
- **NumPy**: Operacje numeryczne
- **Matplotlib**: Wizualizacja danych
- **Multiprocessing**: Obliczenia rÃ³wnolegÅ‚e

## Instalacja i Uruchomienie

```bash
# Instalacja wymaganych bibliotek
pip install tenseal torch scikit-learn numpy matplotlib pandas

# Uruchomienie notebookÃ³w
jupyter notebook sklearn_covtype_project.ipynb
# lub
jupyter notebook torch_project.ipynb
```

## Wnioski KoÅ„cowe

Projekt wykazaÅ‚, Å¼e:

1. **Szyfrowanie homomorficzne jest wykonalne** dla klasyfikacji binarnej, ale wiÄ…Å¼e siÄ™ z znacznym kosztem obliczeniowym
2. **RÃ³wnolegÅ‚oÅ›Ä‡ przynosi korzyÅ›ci** tylko przy duÅ¼ych zbiorach danych (>10k prÃ³bek)
3. **Kompromis prywatnoÅ›Ä‡-wydajnoÅ›Ä‡** jest akceptowalny dla aplikacji krytycznych
4. **Punkt progu opÅ‚acalnoÅ›ci** rÃ³wnolegÅ‚oÅ›ci znajduje siÄ™ miÄ™dzy 200 a 10,000 prÃ³bek

## Autorzy i Licencja

Projekt realizowany w ramach projektu magisterskiego z zakresu **Metod Kryptografii**.

- Mikotaj Stefanski
- Aliaksei Shauchenka
- Kamil Bednarz
- Wojtek Smolarczyk
- Damian Torbus

---
*ğŸ’¡ **Tip**: Dla najlepszych wynikÃ³w uÅ¼ywaj Scikit-Learn z duÅ¼ymi zbiorami danych i wÅ‚Ä…czonÄ… rÃ³wnolegÅ‚oÅ›ciÄ…!*